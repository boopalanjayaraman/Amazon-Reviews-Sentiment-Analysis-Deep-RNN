{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "from string import punctuation\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the json file\n",
    "with open('Amazon_Instant_Video_5.json', 'r') as file:\n",
    "    reviewstext = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert each line text into json and form the json array\n",
    "reviewsjson = []\n",
    "for review in reviewstext.split('\\n'):\n",
    "    if(review != ''):\n",
    "        reviewsjson.append(json.loads(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37126"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviewsjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I had big expectations because I love English TV, in particular Investigative and detective stuff but this guy is really boring. It didn't appeal to me at all.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewsjson[0]['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asin': 'B000H00VBQ',\n",
       " 'helpful': [0, 0],\n",
       " 'overall': 2.0,\n",
       " 'reviewText': \"I had big expectations because I love English TV, in particular Investigative and detective stuff but this guy is really boring. It didn't appeal to me at all.\",\n",
       " 'reviewTime': '05 3, 2014',\n",
       " 'reviewerID': 'A11N155CW1UV02',\n",
       " 'reviewerName': 'AdrianaM',\n",
       " 'summary': 'A little bit boring for me',\n",
       " 'unixReviewTime': 1399075200}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewsjson[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewsjson[0]['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocessing the review texts.\n"
     ]
    }
   ],
   "source": [
    "#a function for replacing characters and converting special form of words\n",
    "def replaceSpecialChars(reviewtext):\n",
    "    reviewtext = reviewtext.replace('.', ' ')\n",
    "    reviewtext = reviewtext.replace(',', ' ')\n",
    "    reviewtext = reviewtext.replace(';', ' ')\n",
    "    reviewtext = reviewtext.replace(':', ' ')\n",
    "    reviewtext = reviewtext.replace(',', ' ')\n",
    "    reviewtext = reviewtext.replace('?', ' ')\n",
    "    reviewtext = reviewtext.replace('!', ' ')\n",
    "    reviewtext = reviewtext.replace('*', ' ')\n",
    "    reviewtext = reviewtext.replace('&', ' ')\n",
    "    reviewtext = reviewtext.replace('(', ' ')\n",
    "    reviewtext = reviewtext.replace(')', ' ')\n",
    "    reviewtext = reviewtext.replace('[', ' ')\n",
    "    reviewtext = reviewtext.replace(']', ' ')\n",
    "    reviewtext = reviewtext.replace('{', ' ')\n",
    "    reviewtext = reviewtext.replace('}', ' ')\n",
    "    reviewtext = reviewtext.replace('/', ' ')\n",
    "    reviewtext = reviewtext.replace('\\\\', ' ')\n",
    "    reviewtext = reviewtext.replace('-', ' ')\n",
    "    reviewtext = reviewtext.replace('+', ' ')\n",
    "    reviewtext = reviewtext.replace('\\'s', ' ')\n",
    "    return reviewtext\n",
    "\n",
    "#prepare all text vocabulary and word2int dictionary, etc\n",
    "allwords = set() # this is the word universe\n",
    "allwords_counter = Counter() # this will be used to clip most frequently occurring and least frequently occurring words\n",
    "positivewords_counter = Counter() #to identify words that occur more frequently in positive reviews\n",
    "negativewords_counter = Counter() #to identify words that occur more frequently in negative reviews\n",
    "for review in reviewsjson:\n",
    "    reviewtext = review['reviewText'].lower()\n",
    "    reviewtext = replaceSpecialChars(reviewtext)\n",
    "    words = reviewtext.split(' ')\n",
    "    allwords.update(words)\n",
    "    allwords_counter.update(words)\n",
    "    if int(review['overall'])>=3: #consider positive\n",
    "        positivewords_counter.update(words)\n",
    "    else:\n",
    "        negativewords_counter.update(words)\n",
    "print('finished preprocessing the review texts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished computing pos to neg ratio correlation.\n"
     ]
    }
   ],
   "source": [
    "#prepare a correlation between positive words and negative words and most common words to both sentiments\n",
    "most_common_words = allwords_counter.most_common()\n",
    "pos_neg_ratio = {}\n",
    "for word, count in most_common_words:\n",
    "    pos_count = positivewords_counter[word]\n",
    "    neg_count = negativewords_counter[word]\n",
    "    pos_neg_ratio[word] = float(pos_count)/float(neg_count+1) #applying log for the values of ratios and subtracting 1 \n",
    "    pos_neg_ratio[word] = np.log(pos_neg_ratio[word]+0.000001)-1 #shifting the axis centre to 1\n",
    "    #now most common words (such as the,was,has,what,is,on,for,of, etc) will fall around 0-1. We can eliminate them\n",
    "print('finished computing pos to neg ratio correlation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:2197\n"
     ]
    }
   ],
   "source": [
    "#prepare vocabulary set\n",
    "vocab = []\n",
    "for word, count in most_common_words:\n",
    "    if (count > 50) & ((pos_neg_ratio[word] >= 1.0) | (pos_neg_ratio[word] < 0.0)):\n",
    "        vocab.append(word)\n",
    "print('vocab length:{0}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preparing vocab_to_int. Length:2197\n"
     ]
    }
   ],
   "source": [
    "#prepare vocabulary to int dictionary\n",
    "vocab_to_int = {word:i for i, word in enumerate(vocab)}\n",
    "print('finished preparing vocab_to_int. Length:{0}'.format(len(vocab_to_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preparing features and labels\n"
     ]
    }
   ],
   "source": [
    "#prepare features and labels\n",
    "reviewsjson_part = reviewsjson[:1280]\n",
    "\n",
    "features = np.zeros((len(reviewsjson_part), len(vocab)))\n",
    "labels = np.zeros((len(reviewsjson_part), 1))\n",
    "\n",
    "pos_label_index = 0\n",
    "neg_label_index = 1\n",
    "\n",
    "def convert_review_to_feature(json):\n",
    "    reviewtext = review['reviewText'].lower()\n",
    "    reviewtext = replaceSpecialChars(reviewtext)\n",
    "    words = reviewtext.split(' ')\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for word in words:\n",
    "        if(word in vocab_to_int):\n",
    "            vec[vocab_to_int[word]] = 1\n",
    "    return vec\n",
    "\n",
    "def convert_review_to_label_onehot(json):\n",
    "    score = int(review['overall'])\n",
    "    vec = np.zeros(2)\n",
    "    if(score >= 3):\n",
    "        vec[pos_label_index] = 1\n",
    "    else:\n",
    "        vec[neg_label_index] = 1\n",
    "    return vec\n",
    "\n",
    "def convert_review_to_label(json):\n",
    "    score = int(review['overall'])\n",
    "    vec = 0\n",
    "    if(score >= 3):\n",
    "        vec = 1\n",
    "    return vec\n",
    "\n",
    "for i, review in enumerate(reviewsjson_part):\n",
    "    features[i] = convert_review_to_feature(review)\n",
    "    labels[i] = convert_review_to_label(review)\n",
    "\n",
    "print('finished preparing features and labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished splitting training and test sets\n"
     ]
    }
   ],
   "source": [
    "#prepare training and testing set\n",
    "train_split_per = 0.8\n",
    "validation_split_per = 0.1\n",
    "features_count = len(features)\n",
    "\n",
    "features_train, features_test = features[: int(train_split_per*features_count)], features[int(train_split_per*features_count):]\n",
    "labels_train, labels_test = labels[: int(train_split_per*features_count)], labels[int(train_split_per*features_count):]\n",
    "\n",
    "print('finished splitting training and test sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished initializing params and creating embedding layers\n"
     ]
    }
   ],
   "source": [
    "#build graph for RNN\n",
    "#uses LSTM memory cell\n",
    "lstm_size = 64\n",
    "lstm_layers = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "# create the input and label placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "#create the first layer (embedding)\n",
    "embed_size = 50 #embedding layer size\n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((len(vocab), embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "\n",
    "print('finished initializing params and creating embedding layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished creating lstm layers\n"
     ]
    }
   ],
   "source": [
    "#create the lstm layer\n",
    "with graph.as_default():\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    #adding a drop out layer\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    #stack up the layer\n",
    "    cell_layers = tf.contrib.rnn.MultiRNNCell([drop]*lstm_layers)\n",
    "    #get the initial state for assigning     \n",
    "    init_state = cell_layers.zero_state(batch_size, tf.float32)\n",
    "\n",
    "print('finished creating lstm layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished creating forward pass functions\n"
     ]
    }
   ],
   "source": [
    "#do the forward pass of RNN\n",
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell_layers, embed, initial_state=init_state)\n",
    "    \n",
    "    #form a regular fully_connected nn layer to obtain predictions\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid) #sigmoid because just two labels\n",
    "    \n",
    "    #cost computation\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    #validation accuracy\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "print('finished creating forward pass functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished creating batching function\n"
     ]
    }
   ],
   "source": [
    "#creating a batching function\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    #get the total number of batches that can be formed\n",
    "    n = len(x) // batch_size\n",
    "    x_clipped = x[:n*batch_size]\n",
    "    y_clipped = y[:n*batch_size]\n",
    "    \n",
    "    batches = []\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        x_batch = x_clipped[i: i+batch_size]\n",
    "        y_batch = y_clipped[i: i+batch_size]\n",
    "        batch = [x_batch, y_batch]\n",
    "        if(len(x_batch) >0):\n",
    "            batches.append(batch)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "print('finished creating batching function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare saver\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2 Iteration: 1 Train loss: 0.382\n",
      "Validation accuracy: 0.023\n",
      "Epoch: 0/2 Iteration: 2 Train loss: 0.273\n",
      "Validation accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "#training function\n",
    "epochs = 2\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    iteration = 1\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        state = session.run(init_state)\n",
    "        \n",
    "        #get the batches\n",
    "        batches = get_batches(features_train, labels_train, batch_size)\n",
    "        \n",
    "        val_batch_index = np.random.randint(0, len(batches))\n",
    "        x_val = batches[val_batch_index][0]\n",
    "        y_val = batches[val_batch_index][1]\n",
    "        \n",
    "        #run for each batch\n",
    "        for i, (x, y) in enumerate(batches, 1):\n",
    "            \n",
    "            if i==val_batch_index:\n",
    "                continue\n",
    "            \n",
    "            #feed dictionary\n",
    "            feed_dict = {inputs_:x, labels_:y, keep_prob:0.75, init_state: state}\n",
    "            \n",
    "            #execute cost, optimizer, and final state\n",
    "            loss, state, _ = session.run([cost, final_state, optimizer], feed_dict=feed_dict)\n",
    "            \n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "                        \n",
    "            #execute cross validation process \n",
    "            feed_dict_val = {inputs_:x_val, labels_:y_val, keep_prob:1, init_state: state}\n",
    "            \n",
    "            #execute val accuracy and final state\n",
    "            batch_acc, val_state = session.run([accuracy, final_state], feed_dict=feed_dict_val)\n",
    "            print(\"Validation accuracy: {:.3f}\".format(batch_acc))\n",
    "            \n",
    "            iteration+=1\n",
    "            \n",
    "    #save the model after every epoch\n",
    "    saver.save(session, \"checkpoints_amazon/sentiment.ckpt\")\n",
    "    \n",
    "print('finished training and saved model in each epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing function\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('checkpoints_amazon'))\n",
    "    \n",
    "    #execute testing, prepare test state\n",
    "    \n",
    "    #get the batches\n",
    "    test_batches = get_batches(features_test, labels_test, batch_size)\n",
    "    \n",
    "    test_acc = []\n",
    "    \n",
    "    for i,(x,y) in enumerate(test_batches, 1):\n",
    "        \n",
    "        feed_dict_test= {inputs_:x, labels_:y, keep_prob:1, init_state: state}\n",
    "        \n",
    "        batch_acc, test_state = session.run([accuracy, final_state], feed_dict=feed_dict_test)\n",
    "        test_acc.append(batch_acc)\n",
    "        print(\"Current batch Test accuracy: {:.3f}\".format(batch_acc))\n",
    "        print(\"Overall test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
